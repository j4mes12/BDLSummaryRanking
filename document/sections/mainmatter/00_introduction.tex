Text summarisation is the process of condensing a passage of text into a shorter version whilst retaining the necessary information in the text. This is a valuable research area since summarisation massively reduces the comprehension time of large pieces of text. Moreover, it has applications in many different domains, both public and professional: academics are required to read extensive research papers, individuals read long articles to keep up to date with the world news, and individuals read books to learn about various topics from history to science.

\medbreak
There are two approaches to text summarisation: extractive and abstractive summarisation. Extractive summarisation is a summarisation technique which focuses on selecting particular words and sentences to convey the meaning of the original text; one would consider a system whereby several sentences are randomly selected to generate summaries an extractive model, albeit not a smart one. Whereas abstractive summarisation techniques look to understand the semantics of the text before generating text to summarise what the model has learnt; an example of such a model is Google's Pegasus text summarisation model \cite{Zhao19}.

\medbreak
Extractive summarisation models are more common as most practical text summarisation models are extractive \cite{Gudivada15}. The basic structure of these models is made up of three stages \cite{Nenkova11}: first, capturing the key aspects of the text; secondly, using these aspects to score the importance of each sentence; thirdly, to create a summary using the highest scoring sentences. Abstractive summarisation is, naturally, harder, and more computationally costly to perform \cite{Gudivada15}. The difficulty is centred on learning the semantics of the text; different texts can have many different structures and models find it difficult to learn such variety \cite{Zhu21}.
		
\medbreak
Passage ranking is a popular technique that is used in various natural language processing (NLP) domains, such as search engine queries \cite{Chang20}, community question answering models \cite{Lin17} and text summarisation models \cite{Simpson19}. In fact, on the 10\textsuperscript{th} of February 2021, Google introduced an update to Google Search which moved their algorithm from a passage indexing approach to a passage ranking approach \cite{Seround21}. We will be utilising this powerful approach to rank summaries by how appropriate they are for the user's requirements. We intend to build up the passage ranking mindset outlined by Simpson et al. \cite{Simpson19} who found this approach useful, but established that a complete ranking was not required. Instead, they concluded that an iterative comparison between the current best summary and a suitable proposed summary was all that was required.

\medbreak
This project focuses on assessing a new approach that uses Bayesian deep learning techniques to rank text summaries generated using extractive text summarisation models. It is necessary for our approach to achieve the following requirements:

\begin{itemize}
	\item {The ability to tailor summaries to the user’s preference since there is a range of requirements that different users have.}
	\item{Of all proposed summaries, the highest-ranked summary should be the most effective at conveying the information in the original text.}
	\item {The framework should be used in an interactive setting which minimises user interactions and a timely processing speed.}
\end{itemize}

\section{Current Approaches}
\label{chap:introduction:curr_approaches}
		
From current literature, models have been proposed to capture the preference of one summary to another such as the Bradley-Terry model \cite{Bradley52} and the Thurstone-Mosteller model \cite{Thurstone27, Mosteller51}. Both the Bradley-Terry and Thurstone-Mosteller models are linear models that takes the "value" of two instances, $a$ and $b$ and represents the probability that $a$ is more comparative than $b$ using a monotonic., increasing function: $\mathcal{P}(a > b) = H(V_a - V_b) $ for "value" variable $V_i$ \cite{Handley01}. These models differ in the functions that are used. The Bradley-Terry model uses a simple additive fraction, $\mathcal{P}(a > b) = \frac{p_a}{p_a + p_b}$ \cite{Hunter04}; whereas the Thurstone-Mosteller model uses  the normal cumulative distribution function for comparison \cite{Handley01}.
		
\medbreak
These models provide good solutions; however, they fail to differentiate between aleatoric and epistemic uncertainty. This limits the models’ ability to determine where there is weakness in the model and leads to reduced performance. Alternative approaches use deep learning techniques to rank passages which beat state-of-the-art performance \cite{Xu19}. However, such models are limited by their requirement of large training data. Which, in the context of text summarisation, comes at a high cost as human annotators are required to manually produce and evaluate summaries. Moreover, these models are unable to account for user preferences which limit the models' ability to tailor summaries to the user.
		
\section{Proposed Approach}
\label{chap:introduction:prop_approach}

\subsection{Research Aim}
\label{chap:introduction:prop_approach:aim}

We aim to develop and evaluate a Bayesian deep learning framework for passage ranking text summaries which incorporates an active learning component to allow for user influence on the summary rankings.
				
\medbreak
Typical deep learning approaches demand vast amounts of training data; however, the active learning component will minimise the number of user interactions required, whilst maintaining high performance. During iterations within the active learning component, we use a stream-based strategy to minimise the amount of overhead processing as, for this strategy, summary instances are evaluated sequentially by the active learner as opposed to requiring a pool of unlabelled instances. As initiated by Simpson et al. \cite{Simpson19}, we will use a Bayesian optimisation acquisition function to determine if an unlabelled instance should be queried by an oracle - an all-knowing information source - or not. We also aim to use Monte Carlo Dropout \cite{Gal15} to approximate the posterior distribution across the model weights and calibrate our model.

\medbreak
Within our framework development and evaluation, we wish to establish if the proposed Bayesian deep learning approach provides a sufficient passage ranking solution in comparison to a classical deep learning model. Moreover, we aim to determine if a stream-based active learning strategy is appropriate for such a problem.
				
\medbreak
This research project will include an experimentation stage; whereby we test the proposed framework, discuss the results and draw conclusions. A range of data sets that have been used for experimentation in summary passage ranking literature: Simpson et al. use the DUC 2001, 2002 and 2004 datasets \cite{Simpson19}; whereas, common benchmarking datasets are the CNN/Daily Mail and GovReport datasets \cite{Nallapati16, Huang21}. We will use one of these datasets to benchmark our results based on which extractive text summarisation model we choose - this will be discussed in Section \ref{chap:literaturereview:summodels}. Since we aim to utilise an active learning component, we will use a noisy random selector to mimic user summary preferences and provide answers to queries; a similar approach was taken by Simpson et al. \cite{Simpson19}.
				
\subsection{Research Concerns}
\label{chap:introduction:prop_approach:concerns}

The central challenge within the project is effectively combining a Bayesian deep learning model with an active learning component. Firstly, it is a concern as to whether stream-based learning is an appropriate active learning strategy for a passage ranking problem since there is minimal current documentation; pool-based active learning is the most common strategy used \cite{Settles09}. Secondly, consideration needs to be made with regard to the number of user interactions. It is necessary for the model to be interactive; thus, it is important to examine the number of interactions that are required and if this is a reasonable level for an interactive setting. Finally, it is a concern as to whether the noisy random simulator effectively represents a human annotator in experimentations. As it will not have a preference for a particular type of summary, it provides little information on whether the framework learns the attributes of a preferred summary and starts to regularly produce summaries of such a nature.
		
		

		
	% -----------------------------------------------------------------------------