%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for James Stephenson at 2022-07-25 18:51:45 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@book{Bhattacharyya20,
	date-added = {2022-07-25 18:51:27 +0100},
	date-modified = {2022-07-25 18:51:41 +0100},
	doi = {doi:10.1515/9783110670905},
	editor = {Siddhartha Bhattacharyya and Vaclav Snasel and Aboul Ella Hassanien and Satadal Saha and B. K. Tripathy},
	isbn = {9783110670905},
	lastchecked = {2022-07-25},
	publisher = {De Gruyter},
	title = {Deep Learning: Research and Applications},
	url = {https://doi.org/10.1515/9783110670905},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1515/9783110670905}}

@article{Amershi14,
	abstract = {Systems that can learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that demonstrate how interactivity results in a tight coupling between the system and the user, exemplify ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. After giving a glimpse of the progress that has been made thus far, we discuss some of the challenges we face in moving the field forward.},
	author = {Amershi, Saleema and Cakmak, Maya and Knox, W. Bradley and Kulesza, Todd},
	date-added = {2022-07-25 15:11:57 +0100},
	date-modified = {2022-07-25 15:12:13 +0100},
	journal = {AI Magazine},
	month = {December},
	publisher = {AAAI - Association for the Advancement of Artificial Intelligence},
	title = {Power to the People: The Role of Humans in Interactive Machine Learning},
	url = {https://www.microsoft.com/en-us/research/publication/power-to-the-people-the-role-of-humans-in-interactive-machine-learning/},
	year = {2014},
	Bdsk-Url-1 = {https://www.microsoft.com/en-us/research/publication/power-to-the-people-the-role-of-humans-in-interactive-machine-learning/}}

@misc{Simpson19,
	author = {Simpson, Edwin and Gao, Yang and Gurevych, Iryna},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-modified = {2022-07-10 19:13:02 +0100},
	doi = {10.48550/ARXIV.1911.10183},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	month = {November},
	publisher = {arXiv},
	title = {Interactive Text Ranking with Bayesian Optimisation: A Case Study on Community QA and Summarisation},
	url = {https://arxiv.org/abs/1911.10183},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1911.10183},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1911.10183}}

@inproceedings{PVS17,
	abstract = {In this paper, we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback. Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS. Our methods complement fully automatic methods in producing high-quality summaries with a minimum number of iterations and feedbacks. We conduct multiple simulation-based experiments and analyze the effect of feedback-based concept selection in the ILP setup in order to maximize the user-desired content in the summary.},
	address = {Vancouver, Canada},
	author = {P.V.S, Avinesh and Meyer, Christian M.},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	doi = {10.18653/v1/P17-1124},
	month = jul,
	pages = {1353--1363},
	publisher = {Association for Computational Linguistics},
	title = {Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback},
	url = {https://aclanthology.org/P17-1124},
	year = {2017},
	Bdsk-Url-1 = {https://aclanthology.org/P17-1124},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P17-1124}}

@misc{Lin17,
	author = {Lin, Xiao and Parikh, Devi},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1711.01732},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Active Learning for Visual Question Answering: An Empirical Study},
	url = {https://arxiv.org/abs/1711.01732},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1711.01732},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1711.01732}}

@article{Bobadilla12,
	abstract = {The new user cold start issue represents a serious problem in recommender systems as it can lead to the loss of new users who decide to stop using the system due to the lack of accuracy in the recommendations received in that first stage in which they have not yet cast a significant number of votes with which to feed the recommender system?s collaborative filtering core. For this reason it is particularly important to design new similarity metrics which provide greater precision in the results offered to users who have cast few votes. This paper presents a new similarity measure perfected using optimization based on neural learning, which exceeds the best results obtained with current metrics. The metric has been tested on the Netflix and Movielens databases, obtaining important improvements in the measures of accuracy, precision and recall when applied to new user cold start situations. The paper includes the mathematical formalization describing how to obtain the main quality measures of a recommender system using leave- one-out cross validation.},
	author = {Jesus Bobadilla Sancho and Fernando Ortega Requena and Antonio Hernando Esteban and Jes{\'u}s Bernal Berm{\'u}dez},
	journal = {Knowledge-Based Systems},
	keywords = {Cold start, Recommender systems, Collaborative filtering, Neural learning, Similarity measures, Leave-one-out-cross validation.},
	month = {February},
	pages = {225--238},
	title = {A collaborative filtering approach to mitigate the new user cold start problem.},
	url = {https://oa.upm.es/15302/},
	volume = {26},
	year = {2012},
	Bdsk-Url-1 = {https://oa.upm.es/15302/}}

@article{Zhu05,
	author = {Zhu, Xiaojin},
	year = {2005},
	month = {01},
	pages = {},
	title = {Semi-Supervised Learning With Graphs}
}

@article{Xu19,
	author = {Xu, Peng and Ma, Xiaofei and Nallapati, Ramesh and Xiang, Bing},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-modified = {2022-07-10 19:13:49 +0100},
	doi = {10.48550/ARXIV.1905.05910},
	journal = {May},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Passage Ranking with Weak Supervision},
	url = {https://arxiv.org/abs/1905.05910},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1905.05910},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1905.05910}}

@misc{Devlin18,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://arxiv.org/abs/1810.04805},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1810.04805},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1810.04805}}

@inproceedings{Peris18,
	abstract = {We study the application of active learning techniques to the translation of unbounded data streams via interactive neural machine translation. The main idea is to select, from an unbounded stream of source sentences, those worth to be supervised by a human agent. The user will interactively translate those samples. Once validated, these data is useful for adapting the neural machine translation model. We propose two novel methods for selecting the samples to be validated. We exploit the information from the attention mechanism of a neural machine translation system. Our experiments show that the inclusion of active learning techniques into this pipeline allows to reduce the effort required during the process, while increasing the quality of the translation system. Moreover, it enables to balance the human effort required for achieving a certain translation quality. Moreover, our neural system outperforms classical approaches by a large margin.},
	address = {Brussels, Belgium},
	author = {Peris, {\'A}lvaro and Casacuberta, Francisco},
	booktitle = {Proceedings of the 22nd Conference on Computational Natural Language Learning},
	doi = {10.18653/v1/K18-1015},
	month = {October},
	pages = {151--160},
	publisher = {Association for Computational Linguistics},
	title = {Active Learning for Interactive Neural Machine Translation of Data Streams},
	url = {https://aclanthology.org/K18-1015},
	year = {2018},
	Bdsk-Url-1 = {https://aclanthology.org/K18-1015},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/K18-1015}}

@misc{Nogueira19,
	author = {Nogueira, Rodrigo and Cho, Kyunghyun},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1901.04085},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Passage Re-ranking with BERT},
	url = {https://arxiv.org/abs/1901.04085},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1901.04085},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1901.04085}}

@inproceedings{Gao18,
	abstract = {We propose a method to perform automatic document summarisation without using reference summaries. Instead, our method interactively learns from users{'} preferences. The merit of preference-based interactive summarisation is that preferences are easier for users to provide than reference summaries. Existing preference-based interactive learning methods suffer from high sample complexity, i.e. they need to interact with the oracle for many rounds in order to converge. In this work, we propose a new objective function, which enables us to leverage active learning, preference learning and reinforcement learning techniques in order to reduce the sample complexity. Both simulation and real-user experiments suggest that our method significantly advances the state of the art. Our source code is freely available at \url{https://github.com/UKPLab/emnlp2018-april}.},
	address = {Brussels, Belgium},
	author = {Gao, Yang and Meyer, Christian M. and Gurevych, Iryna},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	doi = {10.18653/v1/D18-1445},
	month = oct # {-} # nov,
	pages = {4120--4130},
	publisher = {Association for Computational Linguistics},
	title = {{APRIL}: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning},
	url = {https://aclanthology.org/D18-1445},
	year = {2018},
	Bdsk-Url-1 = {https://aclanthology.org/D18-1445},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/D18-1445}}

@inproceedings{Cohn13,
	address = {Sofia, Bulgaria},
	author = {Cohn, Trevor and Specia, Lucia},
	booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	month = aug,
	pages = {32--42},
	publisher = {Association for Computational Linguistics},
	title = {Modelling Annotator Bias with Multi-task {G}aussian Processes: An Application to Machine Translation Quality Estimation},
	url = {https://aclanthology.org/P13-1004},
	year = {2013},
	Bdsk-Url-1 = {https://aclanthology.org/P13-1004}}

@inproceedings{Beck14,
	address = {Doha, Qatar},
	author = {Beck, Daniel and Cohn, Trevor and Specia, Lucia},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	doi = {10.3115/v1/D14-1190},
	month = oct,
	pages = {1798--1803},
	publisher = {Association for Computational Linguistics},
	title = {Joint Emotion Analysis via Multi-task {G}aussian Processes},
	url = {https://aclanthology.org/D14-1190},
	year = {2014},
	Bdsk-Url-1 = {https://aclanthology.org/D14-1190},
	Bdsk-Url-2 = {https://doi.org/10.3115/v1/D14-1190}}

@article{Simpson18,
	abstract = {We introduce a scalable Bayesian preference learning method for identifying convincing arguments in the absence of gold-standard ratings or rankings. In contrast to previous work, we avoid the need for separate methods to perform quality control on training data, predict rankings and perform pairwise classification. Bayesian approaches are an effective solution when faced with sparse or noisy training data, but have not previously been used to identify convincing arguments. One issue is scalability, which we address by developing a stochastic variational inference method for Gaussian process (GP) preference learning. We show how our method can be applied to predict argument convincingness from crowdsourced data, outperforming the previous state-of-the-art, particularly when trained with small amounts of unreliable data. We demonstrate how the Bayesian approach enables more effective active learning, thereby reducing the amount of data required to identify convincing arguments for new users and domains. While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting argument convincingness.},
	address = {Cambridge, MA},
	author = {Simpson, Edwin and Gurevych, Iryna},
	doi = {10.1162/tacl_a_00026},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {357--371},
	publisher = {MIT Press},
	title = {Finding Convincing Arguments Using Scalable {B}ayesian Preference Learning},
	url = {https://aclanthology.org/Q18-1026},
	volume = {6},
	year = {2018},
	Bdsk-Url-1 = {https://aclanthology.org/Q18-1026},
	Bdsk-Url-2 = {https://doi.org/10.1162/tacl_a_00026}}

@misc{Qiao19,
	author = {Qiao, Yifan and Xiong, Chenyan and Liu, Zhenghao and Liu, Zhiyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1904.07531},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Understanding the Behaviors of BERT in Ranking},
	url = {https://arxiv.org/abs/1904.07531},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1904.07531},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1904.07531}}

@article{Mridha21,
	author = {Ph. D., M. and Lima, Aklima and Nur, Kamruddin and Das, Sujoy and Hasan, Mahmud and Kabir, Muhammad},
	doi = {10.1109/ACCESS.2021.3129786},
	journal = {IEEE Access},
	month = {11},
	pages = {1-1},
	title = {A Survey of Automatic Text Summarization: Progress, Process and Challenges},
	volume = {PP},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1109/ACCESS.2021.3129786}}

@article{Lecun15,
	author = {LeCun, Yann and Bengio, Y. and Hinton, Geoffrey},
	doi = {10.1038/nature14539},
	journal = {Nature},
	month = {05},
	pages = {436-44},
	title = {Deep Learning},
	volume = {521},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature14539}}

@article{Azar17,
	author = {Yousefi-Azar, Mahmood and Hamey, Len},
	journal = {Expert Systems with Applications},
	pages = {93--105},
	publisher = {Elsevier},
	title = {Text summarization using unsupervised deep learning},
	volume = {68},
	year = {2017}}

@article{Sharma18,
	author = {Sharma, Yashvardhan and Gupta, Sahil},
	journal = {Procedia computer science},
	pages = {785--794},
	publisher = {Elsevier},
	title = {Deep learning approaches for question answering system},
	volume = {132},
	year = {2018}}

@inproceedings{Hero15,
	author = {Alfred O. Hero},
	title = {STATISTICAL METHODS FOR SIGNAL PROCESSING},
	year = {2005}}

@article{Navin21,
	author = {Sabharwal, Navin and Agrawal, Amit},
	year = {2021},
	month = {01},
	pages = {},
	title = {Hands-on Question Answering Systems with BERT, Applications in Neural Networks and Natural Language Processing},
	isbn = {978-1-4842-6663-2},
	doi = {10.1007/978-1-4842-6664-9}
}

@inproceedings{EinDor20,
	abstract = {Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.},
	address = {Online},
	author = {Ein-Dor, Liat and Halfon, Alon and Gera, Ariel and Shnarch, Eyal and Dankin, Lena and Choshen, Leshem and Danilevsky, Marina and Aharonov, Ranit and Katz, Yoav and Slonim, Noam},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	doi = {10.18653/v1/2020.emnlp-main.638},
	month = nov,
	pages = {7949--7962},
	publisher = {Association for Computational Linguistics},
	title = {{A}ctive {L}earning for {BERT}: {A}n {E}mpirical {S}tudy},
	url = {https://aclanthology.org/2020.emnlp-main.638},
	year = {2020},
	Bdsk-Url-1 = {https://aclanthology.org/2020.emnlp-main.638},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.638}}

@techreport{Settles09,
	added-at = {2011-03-25T11:05:49.000+0100},
	author = {Settles, Burr},
	biburl = {https://www.bibsonomy.org/bibtex/211a6f820b613ae8cacc5cccfe41f6b38/beate},
	institution = {University of Wisconsin--Madison},
	interhash = {d21ffc0eaffcf51e86e81779fe2b22c2},
	intrahash = {11a6f820b613ae8cacc5cccfe41f6b38},
	keywords = {active-learning literature-review spam-detection survey},
	number = 1648,
	timestamp = {2011-03-25T11:05:49.000+0100},
	title = {Active Learning Literature Survey},
	type = {Computer Sciences Technical Report},
	url = {http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf},
	year = 2009,
	Bdsk-Url-1 = {http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf}}

@inproceedings{Zhang19,
	author = {Zhang, Leihan and Zhang, Le},
	doi = {10.1145/3374587.3374611},
	month = {12},
	pages = {107-111},
	title = {An Ensemble Deep Active Learning Method for Intent Classification},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1145/3374587.3374611}}

@misc{Gissin19,
	author = {Gissin, Daniel and Shalev-Shwartz, Shai},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1907.06347},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Discriminative Active Learning},
	url = {https://arxiv.org/abs/1907.06347},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1907.06347},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1907.06347}}

@misc{Gal15,
	author = {Gal, Yarin and Ghahramani, Zoubin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1506.02142},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	url = {https://arxiv.org/abs/1506.02142},
	year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1506.02142},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1506.02142}}

@misc{Izmailov20,
	author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2002.08791},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
	url = {https://arxiv.org/abs/2002.08791},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2002.08791},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.2002.08791}}

@article{Wang20,
	abstract = {A comprehensive artificial intelligence system needs to not only perceive the environment with different ``senses'' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks, such as visual object recognition and speech recognition, using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models.1 In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and, in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, and so on. We also discuss the relationship and differences between Bayesian deep learning and other related topics, such as Bayesian treatment of neural networks.},
	address = {New York, NY, USA},
	articleno = {108},
	author = {Wang, Hao and Yeung, Dit-Yan},
	doi = {10.1145/3409383},
	issn = {0360-0300},
	issue_date = {September 2021},
	journal = {ACM Comput. Surv.},
	keywords = {Bayesian networks, generative models, Deep learning, probabilistic graphical models},
	month = {sep},
	number = {5},
	numpages = {37},
	publisher = {Association for Computing Machinery},
	title = {A Survey on Bayesian Deep Learning},
	url = {https://doi.org/10.1145/3409383},
	volume = {53},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1145/3409383}}

@misc{Wilson20,
	author = {Wilson, Andrew Gordon},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-modified = {2022-07-10 19:12:05 +0100},
	doi = {10.48550/ARXIV.2001.10995},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	month = {January},
	publisher = {arXiv},
	title = {The Case for Bayesian Deep Learning},
	url = {https://arxiv.org/abs/2001.10995},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2001.10995},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.2001.10995}}
