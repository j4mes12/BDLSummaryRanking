%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for James Stephenson at 2022-07-31 16:43:56 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{Mosteller51,
	abstract = {Thurstone's Case V of the method of paired comparisons assumes equal standard deviations of sensations corresponding to stimuli and zero correlations between pairs of stimuli sensations. It is shown that the assumption of zero correlations can be relaxed to an assumption of equal correlations between pairs with no change in method. Further the usual approach to the method of paired comparisons Case V is shown to lead to a least squares estimate of the stimulus positions on the sensation scale.},
	author = {Mosteller, Frederick},
	da = {1951/03/01},
	date-added = {2022-07-31 16:43:47 +0100},
	date-modified = {2022-07-31 16:43:55 +0100},
	doi = {10.1007/BF02313422},
	id = {Mosteller1951},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {3--9},
	title = {Remarks on the method of paired comparisons: I. The least squares solution assuming equal standard deviations and equal correlations},
	ty = {JOUR},
	url = {https://doi.org/10.1007/BF02313422},
	volume = {16},
	year = {1951},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF02313422}}

@article{Thurstone27,
	added-at = {2009-10-20T18:51:19.000+0200},
	author = {Thurstone, Louis Leon},
	biburl = {https://www.bibsonomy.org/bibtex/20c3392ec01bc48cc39fe0415e1caf745/juffi},
	interhash = {6ec2dd7c2638221cbf2c15a42ef7edad},
	intrahash = {0c3392ec01bc48cc39fe0415e1caf745},
	journal = {Psychological Review},
	keywords = {imported},
	pages = {278--286},
	timestamp = {2009-10-20T18:51:19.000+0200},
	title = {A Law of Comparative Judgement},
	volume = 34,
	year = 1927}

@article{Bradley52,
	author = {Ralph Allan Bradley and Milton E. Terry},
	issn = {00063444},
	journal = {Biometrika},
	number = {3/4},
	pages = {324--345},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
	url = {http://www.jstor.org/stable/2334029},
	urldate = {2022-07-31},
	volume = {39},
	year = {1952},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2334029}}

@article{Nenkova11,
	author = {Nenkova, Ani and McKeown, Kathleen},
	date-added = {2022-07-31 15:38:27 +0100},
	date-modified = {2022-07-31 15:38:37 +0100},
	journal = {Foundations and Trends in Information Retrieval},
	month = {06},
	pages = {103-233},
	title = {Automatic summarization},
	volume = {5},
	year = {2011}}

@inproceedings{Gao20,
	abstract = {We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39{\%}. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.},
	address = {Online},
	author = {Gao, Yang and Zhao, Wei and Eger, Steffen},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	doi = {10.18653/v1/2020.acl-main.124},
	month = jul,
	pages = {1347--1354},
	publisher = {Association for Computational Linguistics},
	title = {{SUPERT}: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization},
	url = {https://aclanthology.org/2020.acl-main.124},
	year = {2020},
	Bdsk-Url-1 = {https://aclanthology.org/2020.acl-main.124},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/2020.acl-main.124}}

@article{Tur05,
	abstract = {In this paper, we describe active and semi-supervised learning methods for reducing the labeling effort for spoken language understanding. In a goal-oriented call routing system, understanding the intent of the user can be framed as a classification problem. State of the art statistical classification systems are trained using a large number of human-labeled utterances, preparation of which is labor intensive and time consuming. Active learning aims to minimize the number of labeled utterances by automatically selecting the utterances that are likely to be most informative for labeling. The method for active learning we propose, inspired by certainty-based active learning, selects the examples that the classifier is the least confident about. The examples that are classified with higher confidence scores (hence not selected by active learning) are exploited using two semi-supervised learning methods. The first method augments the training data by using the machine-labeled classes for the unlabeled utterances. The second method instead augments the classification model trained using the human-labeled utterances with the machine-labeled ones in a weighted manner. We then combine active and semi-supervised learning using selectively sampled and automatically labeled data. This enables us to exploit all collected data and alleviates the data imbalance problem caused by employing only active or semi-supervised learning. We have evaluated these active and semi-supervised learning methods with a call classification system used for AT&T customer care. Our results indicate that it is possible to reduce human labeling effort significantly.},
	author = {Gokhan Tur and Dilek Hakkani-T{\"u}r and Robert E. Schapire},
	date-added = {2022-07-30 22:55:59 +0100},
	date-modified = {2022-07-30 22:56:06 +0100},
	doi = {https://doi.org/10.1016/j.specom.2004.08.002},
	issn = {0167-6393},
	journal = {Speech Communication},
	keywords = {Active learning, Semi-supervised learning, Spoken language understanding, Call classification},
	number = {2},
	pages = {171-186},
	title = {Combining active and semi-supervised learning for spoken language understanding},
	url = {https://www.sciencedirect.com/science/article/pii/S0167639304000962},
	volume = {45},
	year = {2005},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S0167639304000962},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.specom.2004.08.002}}

@inproceedings{Thompson99,
	address = {Bled, Slovenia},
	author = {Cynthia A. Thompson and Mary Elaine Califf and Raymond J. Mooney},
	booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning (ICML-99)},
	key = {RAPIER WOLFIE},
	month = {June},
	pages = {406-414},
	title = {Active Learning for Natural Language Parsing and Information Extraction},
	url = {http://www.cs.utexas.edu/users/ai-lab?thompson:ml99},
	year = {1999},
	Bdsk-Url-1 = {http://www.cs.utexas.edu/users/ai-lab?thompson:ml99}}

@incollection{Dagan95,
	abstract = {In many real-world learning tasks, it is expensive to acquire a sufficient number of labeled examples for training. This paper proposes a general method for efficiently training probabilistic classifiers, by selecting for training only the more informative examples in a stream of unlabeled examples. The method, committee-based sampling, evaluates the informativeness of an example by measuring the degree of disagreement between several model variants. These variants (the committee) are drawn randomly from a probability distribution conditioned by the training set selected so far (Monte-Carlo sampling). The method is particularly attractive because it evaluates the expected information gain from a training example implicitly, making the model both easy to implement and generally applicable. We further show how to apply committee-based sampling for training Hidden Markov Model classifiers, which are commonly used for complex classification tasks. The method was implemented and tested for the task of tagging words in natural language sentences with parts-of-speech. Experimental evaluation of committee-based sampling versus standard sequential training showed a substantial improvement in training efficiency.},
	address = {San Francisco (CA)},
	author = {Ido Dagan and Sean P. Engelson},
	booktitle = {Machine Learning Proceedings 1995},
	date-added = {2022-07-30 22:50:51 +0100},
	date-modified = {2022-07-30 22:50:57 +0100},
	doi = {https://doi.org/10.1016/B978-1-55860-377-6.50027-X},
	editor = {Armand Prieditis and Stuart Russell},
	isbn = {978-1-55860-377-6},
	pages = {150-157},
	publisher = {Morgan Kaufmann},
	title = {Committee-Based Sampling For Training Probabilistic Classifiers},
	url = {https://www.sciencedirect.com/science/article/pii/B978155860377650027X},
	year = {1995},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/B978155860377650027X},
	Bdsk-Url-2 = {https://doi.org/10.1016/B978-1-55860-377-6.50027-X}}

@book{Li06,
	author = {Li, X. and Zaiane, O.R. and Li, Z.},
	date-added = {2022-07-30 17:33:21 +0100},
	date-modified = {2022-07-30 17:33:26 +0100},
	isbn = {9783540370260},
	publisher = {Springer Berlin Heidelberg},
	series = {Lecture Notes in Computer Science},
	title = {Advanced Data Mining and Applications: Second International Conference, ADMA 2006, Xi'an, China, August 14-16, 2006, Proceedings},
	url = {https://books.google.co.uk/books?id=DfmSBQAAQBAJ},
	year = {2006},
	Bdsk-Url-1 = {https://books.google.co.uk/books?id=DfmSBQAAQBAJ}}

@article{Kullback51,
	author = {S. Kullback and R. A. Leibler},
	date-added = {2022-07-30 17:24:27 +0100},
	date-modified = {2022-07-30 17:24:34 +0100},
	doi = {10.1214/aoms/1177729694},
	journal = {The Annals of Mathematical Statistics},
	number = {1},
	pages = {79 -- 86},
	publisher = {Institute of Mathematical Statistics},
	title = {{On Information and Sufficiency}},
	url = {https://doi.org/10.1214/aoms/1177729694},
	volume = {22},
	year = {1951},
	Bdsk-Url-1 = {https://doi.org/10.1214/aoms/1177729694}}

@inproceedings{Qin17,
	author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2022-07-30 16:37:19 +0100},
	date-modified = {2022-07-30 16:37:25 +0100},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Improving the Expected Improvement Algorithm},
	url = {https://proceedings.neurips.cc/paper/2017/file/b19aa25ff58940d974234b48391b9549-Paper.pdf},
	volume = {30},
	year = {2017},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2017/file/b19aa25ff58940d974234b48391b9549-Paper.pdf}}

@inproceedings{Mockus75,
	address = {Berlin, Heidelberg},
	author = {Mo{\v{c}}kus, J.},
	booktitle = {Optimization Techniques IFIP Technical Conference Novosibirsk, July 1--7, 1974},
	date-added = {2022-07-30 16:12:41 +0100},
	date-modified = {2022-07-30 16:12:54 +0100},
	editor = {Marchuk, G. I.},
	isbn = {978-3-540-37497-8},
	pages = {400--404},
	publisher = {Springer Berlin Heidelberg},
	title = {On bayesian methods for seeking the extremum},
	year = {1975}}

@inproceedings{Graves11,
	author = {Graves, Alex},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2022-07-29 16:05:07 +0100},
	date-modified = {2022-07-29 16:05:16 +0100},
	editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Practical Variational Inference for Neural Networks},
	url = {https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf},
	volume = {24},
	year = {2011},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf}}

@inproceedings{Maddox19,
	author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2022-07-29 15:22:48 +0100},
	date-modified = {2022-07-29 15:22:56 +0100},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
	url = {https://proceedings.neurips.cc/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf},
	volume = {32},
	year = {2019},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf}}

@inproceedings{Dasgupta07,
	author = {Dasgupta, Sanjoy and Hsu, Daniel J and Monteleoni, Claire},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2022-07-28 19:06:06 +0100},
	date-modified = {2022-07-28 19:06:19 +0100},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {A general agnostic active learning algorithm},
	url = {https://proceedings.neurips.cc/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf},
	volume = {20},
	year = {2007},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf}}

@inproceedings{Seung92,
	abstract = {We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms.},
	address = {New York, NY, USA},
	author = {Seung, H. S. and Opper, M. and Sompolinsky, H.},
	booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
	date-added = {2022-07-28 19:01:49 +0100},
	date-modified = {2022-07-28 19:01:55 +0100},
	doi = {10.1145/130385.130417},
	isbn = {089791497X},
	location = {Pittsburgh, Pennsylvania, USA},
	numpages = {8},
	pages = {287--294},
	publisher = {Association for Computing Machinery},
	series = {COLT '92},
	title = {Query by Committee},
	url = {https://doi.org/10.1145/130385.130417},
	year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1145/130385.130417}}

@article{Mitchell82,
	abstract = {The problem of concept learning, or forming a general description of a class of objects given a set of examples and non-examples, is viewed here as a search problem. Existing programs that generalize from examples are characterized in terms of the classes of search strategies that they employ. Several classes of search strategies are then analyzed and compared in terms of their relative capabilities and computational complexities.},
	author = {Tom M. Mitchell},
	date-added = {2022-07-28 18:54:40 +0100},
	date-modified = {2022-07-28 18:54:48 +0100},
	doi = {https://doi.org/10.1016/0004-3702(82)90040-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {2},
	pages = {203-226},
	title = {Generalization as search},
	url = {https://www.sciencedirect.com/science/article/pii/0004370282900406},
	volume = {18},
	year = {1982},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/0004370282900406},
	Bdsk-Url-2 = {https://doi.org/10.1016/0004-3702(82)90040-6}}

@article{Cohn94,
	abstract = {Active learning differs from ``learning from examples''in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples.},
	author = {Cohn, David and Atlas, Les and Ladner, Richard},
	da = {1994/05/01},
	date-added = {2022-07-28 12:10:15 +0100},
	date-modified = {2022-07-28 12:10:23 +0100},
	doi = {10.1007/BF00993277},
	id = {Cohn1994},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {2},
	pages = {201--221},
	title = {Improving generalization with active learning},
	ty = {JOUR},
	url = {https://doi.org/10.1007/BF00993277},
	volume = {15},
	year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00993277}}

@inproceedings{Baum92,
	author = {Baum, E.B. and Lang, K},
	booktitle = {IEEE Interational Join Conference of Neural Networks},
	date-added = {2022-07-28 12:02:37 +0100},
	date-modified = {2022-07-28 12:04:28 +0100},
	title = {Query learning can work poorly when a human oracle is used.},
	year = {1992}}

@article{Angluin88,
	abstract = {We consider the problem of using queries to learn an unknown concept. Several types of queries are described and studied: membership, equivalence, subset, superset, disjointness, and exhaustiveness queries. Examples are given of efficient learning methods using various subsets of these queries for formal domains, including the regular languages, restricted classes of context-free languages, the pattern languages, and restricted types of prepositional formulas. Some general lower bound techniques are given. Equivalence queries are compared with Valiant's criterion of probably approximately correct identification under random sampling.},
	author = {Angluin, Dana},
	da = {1988/04/01},
	date-added = {2022-07-28 11:53:05 +0100},
	date-modified = {2022-07-28 11:53:11 +0100},
	doi = {10.1023/A:1022821128753},
	id = {Angluin1988},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {4},
	pages = {319--342},
	title = {Queries and Concept Learning},
	ty = {JOUR},
	url = {https://doi.org/10.1023/A:1022821128753},
	volume = {2},
	year = {1988},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1022821128753}}

@book{Bhattacharyya20,
	date-added = {2022-07-25 18:51:27 +0100},
	date-modified = {2022-07-25 18:51:41 +0100},
	doi = {doi:10.1515/9783110670905},
	editor = {Siddhartha Bhattacharyya and Vaclav Snasel and Aboul Ella Hassanien and Satadal Saha and B. K. Tripathy},
	isbn = {9783110670905},
	lastchecked = {2022-07-25},
	publisher = {De Gruyter},
	title = {Deep Learning: Research and Applications},
	url = {https://doi.org/10.1515/9783110670905},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1515/9783110670905}}

@article{Amershi14,
	abstract = {Systems that can learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that demonstrate how interactivity results in a tight coupling between the system and the user, exemplify ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. After giving a glimpse of the progress that has been made thus far, we discuss some of the challenges we face in moving the field forward.},
	author = {Amershi, Saleema and Cakmak, Maya and Knox, W. Bradley and Kulesza, Todd},
	date-added = {2022-07-25 15:11:57 +0100},
	date-modified = {2022-07-25 15:12:13 +0100},
	journal = {AI Magazine},
	month = {December},
	publisher = {AAAI - Association for the Advancement of Artificial Intelligence},
	title = {Power to the People: The Role of Humans in Interactive Machine Learning},
	url = {https://www.microsoft.com/en-us/research/publication/power-to-the-people-the-role-of-humans-in-interactive-machine-learning/},
	year = {2014},
	Bdsk-Url-1 = {https://www.microsoft.com/en-us/research/publication/power-to-the-people-the-role-of-humans-in-interactive-machine-learning/}}

@misc{Simpson19,
	author = {Simpson, Edwin and Gao, Yang and Gurevych, Iryna},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-modified = {2022-07-10 19:13:02 +0100},
	doi = {10.48550/ARXIV.1911.10183},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	month = {November},
	publisher = {arXiv},
	title = {Interactive Text Ranking with Bayesian Optimisation: A Case Study on Community QA and Summarisation},
	url = {https://arxiv.org/abs/1911.10183},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1911.10183},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1911.10183}}

@inproceedings{PVS17,
	abstract = {In this paper, we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback. Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS. Our methods complement fully automatic methods in producing high-quality summaries with a minimum number of iterations and feedbacks. We conduct multiple simulation-based experiments and analyze the effect of feedback-based concept selection in the ILP setup in order to maximize the user-desired content in the summary.},
	address = {Vancouver, Canada},
	author = {P.V.S, Avinesh and Meyer, Christian M.},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	doi = {10.18653/v1/P17-1124},
	month = jul,
	pages = {1353--1363},
	publisher = {Association for Computational Linguistics},
	title = {Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback},
	url = {https://aclanthology.org/P17-1124},
	year = {2017},
	Bdsk-Url-1 = {https://aclanthology.org/P17-1124},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P17-1124}}

@misc{Lin17,
	author = {Lin, Xiao and Parikh, Devi},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1711.01732},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Active Learning for Visual Question Answering: An Empirical Study},
	url = {https://arxiv.org/abs/1711.01732},
	year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1711.01732},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1711.01732}}

@article{Bobadilla12,
	abstract = {The new user cold start issue represents a serious problem in recommender systems as it can lead to the loss of new users who decide to stop using the system due to the lack of accuracy in the recommendations received in that first stage in which they have not yet cast a significant number of votes with which to feed the recommender system?s collaborative filtering core. For this reason it is particularly important to design new similarity metrics which provide greater precision in the results offered to users who have cast few votes. This paper presents a new similarity measure perfected using optimization based on neural learning, which exceeds the best results obtained with current metrics. The metric has been tested on the Netflix and Movielens databases, obtaining important improvements in the measures of accuracy, precision and recall when applied to new user cold start situations. The paper includes the mathematical formalization describing how to obtain the main quality measures of a recommender system using leave- one-out cross validation.},
	author = {Jesus Bobadilla Sancho and Fernando Ortega Requena and Antonio Hernando Esteban and Jes{\'u}s Bernal Berm{\'u}dez},
	journal = {Knowledge-Based Systems},
	keywords = {Cold start, Recommender systems, Collaborative filtering, Neural learning, Similarity measures, Leave-one-out-cross validation.},
	month = {February},
	pages = {225--238},
	title = {A collaborative filtering approach to mitigate the new user cold start problem.},
	url = {https://oa.upm.es/15302/},
	volume = {26},
	year = {2012},
	Bdsk-Url-1 = {https://oa.upm.es/15302/}}

@phdthesis{Zhu05,
	author = {Zhu, Xiaojin},
	date-modified = {2022-07-30 22:57:27 +0100},
	month = {01},
	school = {Carnegie Mellon University},
	title = {Semi-Supervised Learning With Graphs},
	year = {2005}}

@article{Xu19,
	author = {Xu, Peng and Ma, Xiaofei and Nallapati, Ramesh and Xiang, Bing},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-modified = {2022-07-10 19:13:49 +0100},
	doi = {10.48550/ARXIV.1905.05910},
	journal = {May},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Passage Ranking with Weak Supervision},
	url = {https://arxiv.org/abs/1905.05910},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1905.05910},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1905.05910}}

@misc{Devlin18,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1810.04805},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://arxiv.org/abs/1810.04805},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1810.04805},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1810.04805}}

@inproceedings{Peris18,
	abstract = {We study the application of active learning techniques to the translation of unbounded data streams via interactive neural machine translation. The main idea is to select, from an unbounded stream of source sentences, those worth to be supervised by a human agent. The user will interactively translate those samples. Once validated, these data is useful for adapting the neural machine translation model. We propose two novel methods for selecting the samples to be validated. We exploit the information from the attention mechanism of a neural machine translation system. Our experiments show that the inclusion of active learning techniques into this pipeline allows to reduce the effort required during the process, while increasing the quality of the translation system. Moreover, it enables to balance the human effort required for achieving a certain translation quality. Moreover, our neural system outperforms classical approaches by a large margin.},
	address = {Brussels, Belgium},
	author = {Peris, {\'A}lvaro and Casacuberta, Francisco},
	booktitle = {Proceedings of the 22nd Conference on Computational Natural Language Learning},
	doi = {10.18653/v1/K18-1015},
	month = {October},
	pages = {151--160},
	publisher = {Association for Computational Linguistics},
	title = {Active Learning for Interactive Neural Machine Translation of Data Streams},
	url = {https://aclanthology.org/K18-1015},
	year = {2018},
	Bdsk-Url-1 = {https://aclanthology.org/K18-1015},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/K18-1015}}

@misc{Nogueira19,
	author = {Nogueira, Rodrigo and Cho, Kyunghyun},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1901.04085},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Passage Re-ranking with BERT},
	url = {https://arxiv.org/abs/1901.04085},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1901.04085},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1901.04085}}

@inproceedings{Gao18,
	abstract = {We propose a method to perform automatic document summarisation without using reference summaries. Instead, our method interactively learns from users{'} preferences. The merit of preference-based interactive summarisation is that preferences are easier for users to provide than reference summaries. Existing preference-based interactive learning methods suffer from high sample complexity, i.e. they need to interact with the oracle for many rounds in order to converge. In this work, we propose a new objective function, which enables us to leverage active learning, preference learning and reinforcement learning techniques in order to reduce the sample complexity. Both simulation and real-user experiments suggest that our method significantly advances the state of the art. Our source code is freely available at \url{https://github.com/UKPLab/emnlp2018-april}.},
	address = {Brussels, Belgium},
	author = {Gao, Yang and Meyer, Christian M. and Gurevych, Iryna},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	doi = {10.18653/v1/D18-1445},
	month = oct # {-} # nov,
	pages = {4120--4130},
	publisher = {Association for Computational Linguistics},
	title = {{APRIL}: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning},
	url = {https://aclanthology.org/D18-1445},
	year = {2018},
	Bdsk-Url-1 = {https://aclanthology.org/D18-1445},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/D18-1445}}

@inproceedings{Cohn13,
	address = {Sofia, Bulgaria},
	author = {Cohn, Trevor and Specia, Lucia},
	booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	month = aug,
	pages = {32--42},
	publisher = {Association for Computational Linguistics},
	title = {Modelling Annotator Bias with Multi-task {G}aussian Processes: An Application to Machine Translation Quality Estimation},
	url = {https://aclanthology.org/P13-1004},
	year = {2013},
	Bdsk-Url-1 = {https://aclanthology.org/P13-1004}}

@inproceedings{Beck14,
	address = {Doha, Qatar},
	author = {Beck, Daniel and Cohn, Trevor and Specia, Lucia},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	doi = {10.3115/v1/D14-1190},
	month = oct,
	pages = {1798--1803},
	publisher = {Association for Computational Linguistics},
	title = {Joint Emotion Analysis via Multi-task {G}aussian Processes},
	url = {https://aclanthology.org/D14-1190},
	year = {2014},
	Bdsk-Url-1 = {https://aclanthology.org/D14-1190},
	Bdsk-Url-2 = {https://doi.org/10.3115/v1/D14-1190}}

@article{Simpson18,
	abstract = {We introduce a scalable Bayesian preference learning method for identifying convincing arguments in the absence of gold-standard ratings or rankings. In contrast to previous work, we avoid the need for separate methods to perform quality control on training data, predict rankings and perform pairwise classification. Bayesian approaches are an effective solution when faced with sparse or noisy training data, but have not previously been used to identify convincing arguments. One issue is scalability, which we address by developing a stochastic variational inference method for Gaussian process (GP) preference learning. We show how our method can be applied to predict argument convincingness from crowdsourced data, outperforming the previous state-of-the-art, particularly when trained with small amounts of unreliable data. We demonstrate how the Bayesian approach enables more effective active learning, thereby reducing the amount of data required to identify convincing arguments for new users and domains. While word embeddings are principally used with neural networks, our results show that word embeddings in combination with linguistic features also benefit GPs when predicting argument convincingness.},
	address = {Cambridge, MA},
	author = {Simpson, Edwin and Gurevych, Iryna},
	doi = {10.1162/tacl_a_00026},
	journal = {Transactions of the Association for Computational Linguistics},
	pages = {357--371},
	publisher = {MIT Press},
	title = {Finding Convincing Arguments Using Scalable {B}ayesian Preference Learning},
	url = {https://aclanthology.org/Q18-1026},
	volume = {6},
	year = {2018},
	Bdsk-Url-1 = {https://aclanthology.org/Q18-1026},
	Bdsk-Url-2 = {https://doi.org/10.1162/tacl_a_00026}}

@misc{Qiao19,
	author = {Qiao, Yifan and Xiong, Chenyan and Liu, Zhenghao and Liu, Zhiyuan},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1904.07531},
	keywords = {Information Retrieval (cs.IR), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Understanding the Behaviors of BERT in Ranking},
	url = {https://arxiv.org/abs/1904.07531},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1904.07531},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1904.07531}}

@article{Mridha21,
	author = {Ph. D., M. and Lima, Aklima and Nur, Kamruddin and Das, Sujoy and Hasan, Mahmud and Kabir, Muhammad},
	doi = {10.1109/ACCESS.2021.3129786},
	journal = {IEEE Access},
	month = {11},
	pages = {1-1},
	title = {A Survey of Automatic Text Summarization: Progress, Process and Challenges},
	volume = {PP},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1109/ACCESS.2021.3129786}}

@article{Lecun15,
	author = {LeCun, Yann and Bengio, Y. and Hinton, Geoffrey},
	doi = {10.1038/nature14539},
	journal = {Nature},
	month = {05},
	pages = {436-44},
	title = {Deep Learning},
	volume = {521},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature14539}}

@article{Azar17,
	author = {Yousefi-Azar, Mahmood and Hamey, Len},
	journal = {Expert Systems with Applications},
	pages = {93--105},
	publisher = {Elsevier},
	title = {Text summarization using unsupervised deep learning},
	volume = {68},
	year = {2017}}

@article{Sharma18,
	author = {Sharma, Yashvardhan and Gupta, Sahil},
	journal = {Procedia computer science},
	pages = {785--794},
	publisher = {Elsevier},
	title = {Deep learning approaches for question answering system},
	volume = {132},
	year = {2018}}

@inproceedings{Hero15,
	author = {Alfred O. Hero},
	booktitle = {STATISTICAL METHODS FOR SIGNAL PROCESSING},
	title = {STATISTICAL METHODS FOR SIGNAL PROCESSING},
	year = {2005}}

@book{Navin21,
	author = {Sabharwal, Navin and Agrawal, Amit},
	date-modified = {2022-07-30 22:58:12 +0100},
	doi = {10.1007/978-1-4842-6664-9},
	isbn = {978-1-4842-6663-2},
	month = {01},
	publisher = {Apress Berkeley, CA},
	title = {Hands-on Question Answering Systems with BERT, Applications in Neural Networks and Natural Language Processing},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4842-6664-9}}

@inproceedings{EinDor20,
	abstract = {Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.},
	address = {Online},
	author = {Ein-Dor, Liat and Halfon, Alon and Gera, Ariel and Shnarch, Eyal and Dankin, Lena and Choshen, Leshem and Danilevsky, Marina and Aharonov, Ranit and Katz, Yoav and Slonim, Noam},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	doi = {10.18653/v1/2020.emnlp-main.638},
	month = nov,
	pages = {7949--7962},
	publisher = {Association for Computational Linguistics},
	title = {{A}ctive {L}earning for {BERT}: {A}n {E}mpirical {S}tudy},
	url = {https://aclanthology.org/2020.emnlp-main.638},
	year = {2020},
	Bdsk-Url-1 = {https://aclanthology.org/2020.emnlp-main.638},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.638}}

@techreport{Settles09,
	added-at = {2011-03-25T11:05:49.000+0100},
	author = {Settles, Burr},
	biburl = {https://www.bibsonomy.org/bibtex/211a6f820b613ae8cacc5cccfe41f6b38/beate},
	institution = {University of Wisconsin--Madison},
	interhash = {d21ffc0eaffcf51e86e81779fe2b22c2},
	intrahash = {11a6f820b613ae8cacc5cccfe41f6b38},
	keywords = {active-learning literature-review spam-detection survey},
	number = 1648,
	timestamp = {2011-03-25T11:05:49.000+0100},
	title = {Active Learning Literature Survey},
	type = {Computer Sciences Technical Report},
	url = {http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf},
	year = 2009,
	Bdsk-Url-1 = {http://axon.cs.byu.edu/~martinez/classes/778/Papers/settles.activelearning.pdf}}

@inproceedings{Zhang19,
	author = {Zhang, Leihan and Zhang, Le},
	booktitle = {An Ensemble Deep Active Learning Method for Intent Classification},
	doi = {10.1145/3374587.3374611},
	month = {12},
	pages = {107-111},
	title = {An Ensemble Deep Active Learning Method for Intent Classification},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1145/3374587.3374611}}

@misc{Gissin19,
	author = {Gissin, Daniel and Shalev-Shwartz, Shai},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1907.06347},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Discriminative Active Learning},
	url = {https://arxiv.org/abs/1907.06347},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1907.06347},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1907.06347}}

@misc{Gal15,
	author = {Gal, Yarin and Ghahramani, Zoubin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1506.02142},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	url = {https://arxiv.org/abs/1506.02142},
	year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1506.02142},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1506.02142}}

@misc{Izmailov20,
	author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2002.08791},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
	url = {https://arxiv.org/abs/2002.08791},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2002.08791},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.2002.08791}}

@article{Wang20,
	abstract = {A comprehensive artificial intelligence system needs to not only perceive the environment with different ``senses'' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks, such as visual object recognition and speech recognition, using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models.1 In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and, in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, and so on. We also discuss the relationship and differences between Bayesian deep learning and other related topics, such as Bayesian treatment of neural networks.},
	address = {New York, NY, USA},
	articleno = {108},
	author = {Wang, Hao and Yeung, Dit-Yan},
	doi = {10.1145/3409383},
	issn = {0360-0300},
	issue_date = {September 2021},
	journal = {ACM Comput. Surv.},
	keywords = {Bayesian networks, generative models, Deep learning, probabilistic graphical models},
	month = {sep},
	number = {5},
	numpages = {37},
	publisher = {Association for Computing Machinery},
	title = {A Survey on Bayesian Deep Learning},
	url = {https://doi.org/10.1145/3409383},
	volume = {53},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1145/3409383}}

@misc{Wilson20,
	author = {Wilson, Andrew Gordon},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date-modified = {2022-07-10 19:12:05 +0100},
	doi = {10.48550/ARXIV.2001.10995},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	month = {January},
	publisher = {arXiv},
	title = {The Case for Bayesian Deep Learning},
	url = {https://arxiv.org/abs/2001.10995},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2001.10995},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.2001.10995}}

@book{Brooks11,
	added-at = {2015-03-24T17:28:34.000+0100},
	author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	biburl = {https://www.bibsonomy.org/bibtex/22b8d02bec832fa945b62ecf7808614bf/becker},
	interhash = {0b127e40d41a970274484b65a7e0744f},
	intrahash = {2b8d02bec832fa945b62ecf7808614bf},
	keywords = {carlo chain diss handbook inthesis markov mcmc monte},
	publisher = {CRC press},
	timestamp = {2017-08-04T09:03:42.000+0200},
	title = {Handbook of Markov Chain Monte Carlo},
	year = 2011}

@inproceedings{Neal95,
	author = {Radford M. Neal},
	booktitle = {Bayesian Learning for Neural Networks},
	title = {Bayesian Learning for Neural Networks},
	year = {1995}}

@article{Duane87,
	abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
	added-at = {2009-04-13T13:17:23.000+0200},
	author = {Duane, Simon and Kennedy, A. D. and Pendleton, Brian J. and Roweth, Duncan},
	biburl = {https://www.bibsonomy.org/bibtex/254b1c26b36f9c8a833153a78748a049f/gber},
	description = {ScienceDirect - Physics Letters B : Hybrid Monte Carlo},
	doi = {DOI: 10.1016/0370-2693(87)91197-X},
	interhash = {65f0469e34e74def745a331703f25333},
	intrahash = {54b1c26b36f9c8a833153a78748a049f},
	issn = {0370-2693},
	journal = {Physics Letters B},
	keywords = {Fermions HMC Lattice},
	number = 2,
	pages = {216 - 222},
	timestamp = {2009-04-13T13:17:23.000+0200},
	title = {Hybrid Monte Carlo},
	url = {http://www.sciencedirect.com/science/article/B6TVN-46YSWPH-2XF/2/0f89cdc6cf214a2169b03df7414f3df4},
	volume = 195,
	year = 1987,
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/B6TVN-46YSWPH-2XF/2/0f89cdc6cf214a2169b03df7414f3df4},
	Bdsk-Url-2 = {https://doi.org/10.1016/0370-2693(87)91197-X}}

@article{Srivastava14,
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	issn = {1532-4435},
	issue_date = {January 2014},
	journal = {J. Mach. Learn. Res.},
	keywords = {neural networks, regularization, model combination, deep learning},
	month = {jan},
	number = {1},
	numpages = {30},
	pages = {1929--1958},
	publisher = {JMLR.org},
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	volume = {15},
	year = {2014}}

@misc{Dmitrii18,
	author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1803.05407},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Averaging Weights Leads to Wider Optima and Better Generalization},
	url = {https://arxiv.org/abs/1803.05407},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1803.05407},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1803.05407}}

@misc{He15,
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1512.03385},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Deep Residual Learning for Image Recognition},
	url = {https://arxiv.org/abs/1512.03385},
	year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1512.03385},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.1512.03385}}

@misc{Lewis94,
	author = {Lewis, David D. and Gale, William A.},
	copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
	doi = {10.48550/ARXIV.CMP-LG/9407020},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {A Sequential Algorithm for Training Text Classifiers},
	url = {https://arxiv.org/abs/cmp-lg/9407020},
	year = {1994},
	Bdsk-Url-1 = {https://arxiv.org/abs/cmp-lg/9407020},
	Bdsk-Url-2 = {https://doi.org/10.48550/ARXIV.CMP-LG/9407020}}

@article{Shannon48,
	added-at = {2021-09-19T18:40:37.000+0200},
	author = {Shannon, Claude Elwood},
	biburl = {https://www.bibsonomy.org/bibtex/29f88587b33c82f692b61d129eb2f2517/steschum},
	interhash = {754130207906fcec16a53d330eeff348},
	intrahash = {9f88587b33c82f692b61d129eb2f2517},
	journal = {The Bell System Technical Journal},
	keywords = {imported},
	pages = {379--423},
	timestamp = {2021-09-19T18:41:56.000+0200},
	title = {A Mathematical Theory of Communication},
	url = {http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
	urldate = {2003-04-22},
	volume = 27,
	year = 1948,
	Bdsk-Url-1 = {http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf}}

@inproceedings{Craven08,
	abstract = {Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.},
	address = {USA},
	author = {Settles, Burr and Craven, Mark},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	location = {Honolulu, Hawaii},
	numpages = {10},
	pages = {1070--1079},
	publisher = {Association for Computational Linguistics},
	series = {EMNLP '08},
	title = {An Analysis of Active Learning Strategies for Sequence Labeling Tasks},
	year = {2008}}

@article{Hwa04,
	author = {Hwa, Rebecca},
	doi = {10.1162/0891201041850894},
	journal = {Computational Linguistics},
	number = {3},
	pages = {253--276},
	title = {Sample Selection for Statistical Parsing},
	url = {https://aclanthology.org/J04-3001},
	volume = {30},
	year = {2004},
	Bdsk-Url-1 = {https://aclanthology.org/J04-3001},
	Bdsk-Url-2 = {https://doi.org/10.1162/0891201041850894}}

@article{Sharma17,
	author = {Sharma, Manali and Bilgic, Mustafa},
	doi = {10.1007/s10618-016-0460-3},
	issn = {1573-756X},
	journal = {Data Mining and Knowledge Discovery},
	number = {1},
	pages = {164--202},
	title = {Evidence-based uncertainty sampling for active learning},
	url = {http://www.cs.iit.edu/~ml/pdfs/sharma-dmkd17.pdf},
	volume = {31},
	year = {2017},
	Bdsk-Url-1 = {http://www.cs.iit.edu/~ml/pdfs/sharma-dmkd17.pdf},
	Bdsk-Url-2 = {https://doi.org/10.1007/s10618-016-0460-3}}

@inproceedings{Nigam98,
	added-at = {2008-10-07T16:03:39.000+0200},
	author = {McCallum, A. and Nigam, K.},
	biburl = {https://www.bibsonomy.org/bibtex/2fbfb7ca3e92332e6566b27bdc70e9090/brefeld},
	booktitle = {Proceedings of the International Conference on Machine Learning},
	date-modified = {2022-07-30 22:51:19 +0100},
	interhash = {6ac097d12ca056b9831ce361d5b06299},
	intrahash = {fbfb7ca3e92332e6566b27bdc70e9090},
	keywords = {imported},
	timestamp = {2008-10-07T16:03:42.000+0200},
	title = {Employing EM and Pool-Based Active learning for Text Classification},
	year = 1998}

@misc{Zhao19,
	archiveprefix = {arXiv},
	author = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
	eprint = {1912.08777},
	primaryclass = {cs.CL},
	title = {PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
	year = {2019}}

@article{Lewis19,
	author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
	eprint = {1910.13461},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},
	title = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
	url = {http://arxiv.org/abs/1910.13461},
	volume = {abs/1910.13461},
	year = {2019},
	Bdsk-Url-1 = {http://arxiv.org/abs/1910.13461}}

@inproceedings{Gal17,
	abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
	author = {Yarin Gal and Riashat Islam and Zoubin Ghahramani},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = {06--11 Aug},
	pages = {1183--1192},
	pdf = {http://proceedings.mlr.press/v70/gal17a/gal17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Deep {B}ayesian Active Learning with Image Data},
	url = {https://proceedings.mlr.press/v70/gal17a.html},
	volume = {70},
	year = {2017},
	Bdsk-Url-1 = {https://proceedings.mlr.press/v70/gal17a.html}}

@inproceedings{Tong01,
	author = {Daphne Koller and Simon Tong},
	booktitle = {Active learning: theory and applications},
	title = {Active learning: theory and applications},
	year = {2001}}

@article{Houlsby11,
	added-at = {2018-08-13T00:00:00.000+0200},
	author = {Houlsby, Neil and Huszar, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'a}t{\'e}},
	biburl = {https://www.bibsonomy.org/bibtex/25608791e0d0e2e7c3cd72a91ea3eb931/dblp},
	ee = {http://arxiv.org/abs/1112.5745},
	interhash = {ca9810b9189e9c2b90f5d99a3ebd12cf},
	intrahash = {5608791e0d0e2e7c3cd72a91ea3eb931},
	journal = {CoRR},
	keywords = {dblp},
	timestamp = {2018-08-14T14:26:21.000+0200},
	title = {Bayesian Active Learning for Classification and Preference Learning},
	url = {http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745},
	volume = {abs/1112.5745},
	year = 2011,
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745}}
